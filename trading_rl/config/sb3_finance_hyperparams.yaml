# Reasonable starting hyperparams for trading-like continuous-control tasks.
# Designed to plug into the runner and ALGO_REGISTRY factories (params dict).
#
# Notes:
# - PPO/A2C are on-policy; SAC/TD3 are off-policy.
# - policy_kwargs.activation_fn expects a torch.nn module class; use a string
#   name (e.g. "ReLU") and the runner will resolve it.

shared:
  seed: 42
  policy: MlpPolicy
  policy_kwargs:
    net_arch: [256, 256]
    activation_fn: ReLU
  device: auto
  verbose: 1

ppo:
  learning_rate: 3.0e-4
  n_steps: 512
  batch_size: 256
  n_epochs: 3
  gamma: 0.998
  gae_lambda: 0.95
  clip_range: 0.2
  ent_coef: 0.001
  vf_coef: 0.5
  max_grad_norm: 0.5

a2c:
  learning_rate: 7.0e-4
  n_steps: 5
  gamma: 0.99
  gae_lambda: 1.0
  ent_coef: 0.01
  vf_coef: 0.5
  max_grad_norm: 0.5

sac:
  learning_rate: 3.0e-4
  buffer_size: 1000000
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  ent_coef: auto
  train_freq: 1
  gradient_steps: 2
  learning_starts: 50000

td3:
  learning_rate: 3.0e-4
  buffer_size: 1000000
  batch_size: 256
  gamma: 0.99
  tau: 0.005
  train_freq: 1
  gradient_steps: 1
  learning_starts: 10000
  policy_delay: 2
  target_policy_noise: 0.2
  target_noise_clip: 0.5

vecnormalize:
  enable: true
  norm_obs: true
  norm_reward: false
  clip_obs: 10.0

env:
  trading_cost_pct: 0.0001
  reward_mode: diff_return
  reward_scaling: 1000.0
  window_size: 256
  random_start: true
